{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eb226793",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- CPU Matrix Multiplication ---\n",
      "11.1 ms ± 1.88 ms per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def cpu_matrix_multiplication(A, B):\n",
    "\n",
    "  return np.dot(A, B)\n",
    "\n",
    "\n",
    "A = np.random.rand(1024, 1024).astype(np.float32)\n",
    "B = np.random.rand(1024, 1024).astype(np.float32)\n",
    "\n",
    "print(\"--- CPU Matrix Multiplication ---\")\n",
    "%timeit cpu_matrix_multiplication(A, B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f7c23bca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- GPU Matrix Multiplication ---\n",
      "40.4 ms ± 2.03 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "from numba import cuda\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "@cuda.jit\n",
    "def gpu_matrix_multiplication_kernel(A, B, C):\n",
    "  \"\"\"\n",
    "  CUDA kernel for matrix multiplication.\n",
    "  \"\"\"\n",
    "  row, col = cuda.grid(2)\n",
    "  if row < C.shape[0] and col < C.shape[1]:\n",
    "    tmp = 0.\n",
    "    for k in range(A.shape[1]):\n",
    "      tmp += A[row, k] * B[k, col]\n",
    "    C[row, col] = tmp\n",
    "\n",
    "def gpu_matrix_multiplication(A, B):\n",
    "  \"\"\"\n",
    "  Sets up and runs the CUDA kernel for matrix multiplication.\n",
    "  \"\"\"\n",
    "  # Allocate memory on the device\n",
    "  A_device = cuda.to_device(A)\n",
    "  B_device = cuda.to_device(B)\n",
    "  C_device = cuda.device_array_like(A)\n",
    "\n",
    "  # Configure the blocks and grids\n",
    "  threadsperblock = (32, 32)\n",
    "  blockspergrid_x = math.ceil(A.shape[0] / threadsperblock[0])\n",
    "  blockspergrid_y = math.ceil(B.shape[1] / threadsperblock[1])\n",
    "  blockspergrid = (blockspergrid_x, blockspergrid_y)\n",
    "\n",
    "  # Launch the kernel\n",
    "  gpu_matrix_multiplication_kernel[blockspergrid, threadsperblock](A_device, B_device, C_device)\n",
    "\n",
    "  # Copy the result back to the host\n",
    "  return C_device.copy_to_host()\n",
    "\n",
    "\n",
    "# --- Benchmarking ---\n",
    "# Create two large matrices\n",
    "A = np.random.rand(1024, 1024).astype(np.float32)\n",
    "B = np.random.rand(1024, 1024).astype(np.float32)\n",
    "\n",
    "print(\"\\n--- GPU Matrix Multiplication ---\")\n",
    "%timeit gpu_matrix_multiplication(A, B)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fce600a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- CPU 1D Convolution ---\n",
      "23.8 ms ± 1.51 ms per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def cpu_convolution(signal, kernel):\n",
    "  \"\"\"\n",
    "  Performs 1D convolution on the CPU using NumPy.\n",
    "  \"\"\"\n",
    "  return np.convolve(signal, kernel, mode='same')\n",
    "\n",
    "# --- Benchmarking ---\n",
    "signal = np.random.rand(1000000).astype(np.float32)\n",
    "kernel = np.random.rand(256).astype(np.float32)\n",
    "\n",
    "print(\"--- CPU 1D Convolution ---\")\n",
    "%timeit cpu_convolution(signal, kernel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2b2f857b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- GPU 1D Convolution ---\n",
      "12.2 ms ± 557 μs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "from numba import cuda\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "@cuda.jit\n",
    "def gpu_convolution_kernel(signal, kernel, output):\n",
    "  \"\"\"\n",
    "  CUDA kernel for 1D convolution.\n",
    "  \"\"\"\n",
    "  idx = cuda.grid(1)\n",
    "  kernel_radius = kernel.shape[0] // 2\n",
    "  if idx < output.shape[0]:\n",
    "    temp = 0.\n",
    "    for i in range(kernel.shape[0]):\n",
    "      k_idx = i - kernel_radius\n",
    "      s_idx = idx + k_idx\n",
    "      if s_idx >= 0 and s_idx < signal.shape[0]:\n",
    "        temp += signal[s_idx] * kernel[i]\n",
    "    output[idx] = temp\n",
    "\n",
    "def gpu_convolution(signal, kernel):\n",
    "  \"\"\"\n",
    "  Sets up and runs the CUDA kernel for 1D convolution.\n",
    "  \"\"\"\n",
    "  output = np.zeros_like(signal)\n",
    "  signal_device = cuda.to_device(signal)\n",
    "  kernel_device = cuda.to_device(kernel)\n",
    "  output_device = cuda.to_device(output)\n",
    "\n",
    "  threadsperblock = 256\n",
    "  blockspergrid = math.ceil(signal.shape[0] / threadsperblock)\n",
    "\n",
    "  gpu_convolution_kernel[blockspergrid, threadsperblock](signal_device, kernel_device, output_device)\n",
    "\n",
    "  return output_device.copy_to_host()\n",
    "\n",
    "# --- Benchmarking ---\n",
    "signal = np.random.rand(1000000).astype(np.float32)\n",
    "kernel = np.random.rand(256).astype(np.float32)\n",
    "\n",
    "print(\"\\n--- GPU 1D Convolution ---\")\n",
    "%timeit gpu_convolution(signal, kernel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8af91e63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- CPU Moving Average ---\n",
      "129 ms ± 5.3 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def cpu_moving_average(data, window_size):\n",
    "  \"\"\"\n",
    "  Calculates the moving average on the CPU using NumPy.\n",
    "  \"\"\"\n",
    "  return np.convolve(data, np.ones(window_size), 'valid') / window_size\n",
    "\n",
    "# --- Benchmarking ---\n",
    "data = np.random.rand(5000000).astype(np.float32)\n",
    "window_size = 100\n",
    "\n",
    "print(\"--- CPU Moving Average ---\")\n",
    "%timeit cpu_moving_average(data, window_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d37670c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- GPU Moving Average ---\n",
      "31 ms ± 1.32 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "from numba import cuda\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "@cuda.jit\n",
    "def gpu_moving_average_kernel(data, window_size, output):\n",
    "  \"\"\"\n",
    "  CUDA kernel for calculating the moving average.\n",
    "  \"\"\"\n",
    "  idx = cuda.grid(1)\n",
    "  if idx < output.shape[0]:\n",
    "    temp = 0.\n",
    "    for i in range(window_size):\n",
    "      temp += data[idx + i]\n",
    "    output[idx] = temp / window_size\n",
    "\n",
    "def gpu_moving_average(data, window_size):\n",
    "  \"\"\"\n",
    "  Sets up and runs the CUDA kernel for moving average.\n",
    "  \"\"\"\n",
    "  output = np.zeros(data.shape[0] - window_size + 1, dtype=np.float32)\n",
    "  data_device = cuda.to_device(data)\n",
    "  output_device = cuda.to_device(output)\n",
    "\n",
    "  threadsperblock = 256\n",
    "  blockspergrid = math.ceil(output.shape[0] / threadsperblock)\n",
    "\n",
    "  gpu_moving_average_kernel[blockspergrid, threadsperblock](data_device, window_size, output_device)\n",
    "\n",
    "  return output_device.copy_to_host()\n",
    "\n",
    "# --- Benchmarking ---\n",
    "data = np.random.rand(5000000).astype(np.float32)\n",
    "window_size = 100\n",
    "\n",
    "print(\"\\n--- GPU Moving Average ---\")\n",
    "%timeit gpu_moving_average(data, window_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cbdf7452",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- CPU Correlation ---\n",
      "1.49 ms ± 130 μs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def cpu_correlation(signal1, signal2):\n",
    "  \"\"\"\n",
    "  Performs 1D cross-correlation on the CPU using NumPy.\n",
    "  'full' mode provides the complete cross-correlation.\n",
    "  \"\"\"\n",
    "  return np.correlate(signal1, signal2, mode='full')\n",
    "\n",
    "# --- Benchmarking ---\n",
    "signal1 = np.random.randn(5000).astype(np.float32)\n",
    "signal2 = np.random.randn(5000).astype(np.float32)\n",
    "\n",
    "print(\"--- CPU Correlation ---\")\n",
    "%timeit cpu_correlation(signal1, signal2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0edccebd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- GPU Correlation ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Ssn\\anaconda3\\Lib\\site-packages\\numba\\cuda\\dispatcher.py:536: NumbaPerformanceWarning: \u001b[1mGrid size 40 will likely result in GPU under-utilization due to low occupancy.\u001b[0m\n",
      "  warn(NumbaPerformanceWarning(msg))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.89 ms ± 688 μs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "from numba import cuda\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "@cuda.jit\n",
    "def gpu_correlation_kernel(signal1, signal2, output):\n",
    "  \"\"\"\n",
    "  CUDA kernel for 1D cross-correlation.\n",
    "  \"\"\"\n",
    "  lag = cuda.grid(1)\n",
    "  \n",
    "  if lag < output.shape[0]:\n",
    "    # The starting point in the output corresponds to a specific overlap\n",
    "    n_signal1 = signal1.shape[0]\n",
    "    n_signal2 = signal2.shape[0]\n",
    "    \n",
    "    start_index = lag - (n_signal2 - 1)\n",
    "    \n",
    "    # Calculate the sum of products for the current lag\n",
    "    corr_sum = 0.0\n",
    "    for i in range(n_signal1):\n",
    "      j = i - start_index\n",
    "      if 0 <= j < n_signal2:\n",
    "        corr_sum += signal1[i] * signal2[j]\n",
    "    output[lag] = corr_sum\n",
    "\n",
    "def gpu_correlation(signal1, signal2):\n",
    "  \"\"\"\n",
    "  Sets up and runs the CUDA kernel for 1D correlation.\n",
    "  \"\"\"\n",
    "  output_size = signal1.shape[0] + signal2.shape[0] - 1\n",
    "  output = np.zeros(output_size, dtype=np.float32)\n",
    "  \n",
    "  d_signal1 = cuda.to_device(signal1)\n",
    "  d_signal2 = cuda.to_device(signal2)\n",
    "  d_output = cuda.to_device(output)\n",
    "\n",
    "  threadsperblock = 256\n",
    "  blockspergrid = math.ceil(output_size / threadsperblock)\n",
    "\n",
    "  gpu_correlation_kernel[blockspergrid, threadsperblock](d_signal1, d_signal2, d_output)\n",
    "\n",
    "  return d_output.copy_to_host()\n",
    "\n",
    "# --- Benchmarking ---\n",
    "signal1 = np.random.randn(5000).astype(np.float32)\n",
    "signal2 = np.random.randn(5000).astype(np.float32)\n",
    "\n",
    "print(\"\\n--- GPU Correlation ---\")\n",
    "%timeit gpu_correlation(signal1, signal2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac770831",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import integrate\n",
    "import numpy as np\n",
    "\n",
    "# A sample complex function to integrate\n",
    "def my_func(x):\n",
    "  return np.sin(x**2) * np.exp(-x/2)\n",
    "\n",
    "# --- Benchmarking ---\n",
    "# integrate.quad is not easily benchmarked with %timeit for a single run,\n",
    "# but a typical execution is very fast for simple functions.\n",
    "# We will compare a simple trapezoidal rule on the CPU for a fairer comparison\n",
    "# against the GPU's trapezoidal rule.\n",
    "\n",
    "def cpu_trapezoid_integration(func, a, b, n=10_000_000):\n",
    "    x = np.linspace(a, b, n, dtype=np.float32)\n",
    "    y = func(x)\n",
    "    return np.trapz(y, x)\n",
    "\n",
    "print(\"--- CPU Numerical Integration (Trapezoid Rule) ---\")\n",
    "%timeit cpu_trapezoid_integration(my_func, 0, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db51a302",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numba import cuda\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "@cuda.jit\n",
    "def gpu_trapezoid_kernel(func, a, b, n, partial_sums):\n",
    "    \"\"\"\n",
    "    CUDA kernel to compute the area of many small trapezoids in parallel.\n",
    "    \"\"\"\n",
    "    thread_id = cuda.grid(1)\n",
    "    threads_total = cuda.gridsize(1)\n",
    "\n",
    "    # Each thread computes its share of the trapezoids\n",
    "    dx = (b - a) / n\n",
    "    local_sum = 0.0\n",
    "    \n",
    "    for i in range(thread_id, n, threads_total):\n",
    "        x1 = a + i * dx\n",
    "        x2 = a + (i + 1) * dx\n",
    "        \n",
    "        # Area of one trapezoid\n",
    "        local_sum += (func(x1) + func(x2)) * dx / 2.0\n",
    "        \n",
    "    partial_sums[thread_id] = local_sum\n",
    "\n",
    "def gpu_integration(func, a, b, n=10_000_000):\n",
    "  \"\"\"\n",
    "  Sets up and runs the CUDA kernel for trapezoidal integration.\n",
    "  \"\"\"\n",
    "  # We need to compile the function for the device\n",
    "  device_func = cuda.jit(func, device=True)\n",
    "\n",
    "  threadsperblock = 256\n",
    "  blockspergrid = 32 # Use a fixed number of blocks for this example\n",
    "  \n",
    "  partial_sums = cuda.device_array(threadsperblock * blockspergrid, dtype=np.float32)\n",
    "\n",
    "  gpu_trapezoid_kernel[blockspergrid, threadsperblock](device_func, \n",
    "                                                       np.float32(a), \n",
    "                                                       np.float32(b), \n",
    "                                                       n, \n",
    "                                                       partial_sums)\n",
    "\n",
    "  # Final sum is done on the host for simplicity, but can also be done on device\n",
    "  return partial_sums.copy_to_host().sum()\n",
    "\n",
    "# --- Benchmarking ---\n",
    "print(\"\\n--- GPU Numerical Integration ---\")\n",
    "%timeit gpu_integration(my_func, 0, 10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
